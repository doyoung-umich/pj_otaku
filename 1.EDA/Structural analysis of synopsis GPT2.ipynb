{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb3c5b0",
   "metadata": {},
   "source": [
    "## Structural analysis of synopsis (GPT2)\n",
    "\n",
    "This notebook aims to visualize structural similarities among the anime/comic titles by looking at the words that are used in the synopsis text. It uses GPT2 to vectorize each word from the synopsis text, and for each title, we will take the average of all the word vectors in one title's synopsis and use that as the representative vector of that title.\n",
    "\n",
    "Why GPT2? -> Recommendation in the RoBERTa's \"intended uses\": https://huggingface.co/roberta-base\n",
    "\n",
    "Quote: `tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409041d",
   "metadata": {},
   "source": [
    "#### ToDo:\n",
    "\n",
    "- [x] Create anime/manga flag to meta labels\n",
    "- [x] Try other BERT models\n",
    "- [ ] Structure of table data with standard dim reduction (PCA/UMAP) and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad86cdc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: tensorboardX in /opt/conda/lib/python3.7/site-packages (2.5.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tensorboardX) (1.21.6)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorboardX) (3.19.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorboardX\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bba839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import tensorboard\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e279c6f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import GPT2Tokenizer, GPT2Model\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023d7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_label_array(df, excluded_idx, columns):\n",
    "    flag_list = []\n",
    "    for col in columns:\n",
    "        flag_list.append(np.expand_dims(np.array(df[col]), axis=1))\n",
    "    meta = np.concatenate(flag_list, axis=1)\n",
    "    meta_header = np.insert(meta, 0, columns, axis=0)\n",
    "    meta_header = np.delete(meta_header, excluded_idx, axis=0)\n",
    "    return meta_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f061786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_id</th>\n",
       "      <th>title_english</th>\n",
       "      <th>title_romaji</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>start_year</th>\n",
       "      <th>chapters</th>\n",
       "      <th>volume</th>\n",
       "      <th>publishing_status</th>\n",
       "      <th>country</th>\n",
       "      <th>...</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Slice of Life</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Supernatural</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>title_romaji_type</th>\n",
       "      <th>synopsis_cleaned</th>\n",
       "      <th>synopsis_source</th>\n",
       "      <th>synopsis_wc</th>\n",
       "      <th>synopsis_cleaned_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30002</td>\n",
       "      <td>Berserk</td>\n",
       "      <td>Berserk</td>\n",
       "      <td>MANGA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RELEASING</td>\n",
       "      <td>JP</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Berserk_MANGA</td>\n",
       "      <td>His name Guts Black Swordsman feared warrior s...</td>\n",
       "      <td>Dark Horse</td>\n",
       "      <td>425</td>\n",
       "      <td>['name', 'feared', 'warrior', 'spoken', 'whisp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JoJo no Kimyou na Bouken: Steel Ball Run</td>\n",
       "      <td>MANGA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>JP</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>JoJo no Kimyou na Bouken: Steel Ball Run_MANGA</td>\n",
       "      <td>Originally presented unrelated story series la...</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>346</td>\n",
       "      <td>['presented', 'unrelated', 'story', 'series', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114129</td>\n",
       "      <td>Gintama: THE VERY FINAL</td>\n",
       "      <td>Gintama: THE FINAL</td>\n",
       "      <td>ANIME</td>\n",
       "      <td>104.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>JP</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Gintama: THE FINAL_ANIME</td>\n",
       "      <td>Gintama THE FINAL rd final film adaptation rem...</td>\n",
       "      <td>no match</td>\n",
       "      <td>82</td>\n",
       "      <td>['rd', 'final', 'film', 'adaptation', 'remaind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30013</td>\n",
       "      <td>One Piece</td>\n",
       "      <td>ONE PIECE</td>\n",
       "      <td>MANGA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RELEASING</td>\n",
       "      <td>JP</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ONE PIECE_MANGA</td>\n",
       "      <td>As child Monkey D Luffy inspired become pirate...</td>\n",
       "      <td>Viz Media</td>\n",
       "      <td>348</td>\n",
       "      <td>['child', 'inspired', 'become', 'pirate', 'lis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124194</td>\n",
       "      <td>Fruits Basket The Final Season</td>\n",
       "      <td>Fruits Basket: The Final</td>\n",
       "      <td>ANIME</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>JP</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Fruits Basket: The Final_ANIME</td>\n",
       "      <td>After last season revelations Soma family move...</td>\n",
       "      <td>Funimation</td>\n",
       "      <td>277</td>\n",
       "      <td>['last', 'season', 'revelations', 'family', 'm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   title_id                   title_english  \\\n",
       "0     30002                         Berserk   \n",
       "1     31706                             NaN   \n",
       "2    114129         Gintama: THE VERY FINAL   \n",
       "3     30013                       One Piece   \n",
       "4    124194  Fruits Basket The Final Season   \n",
       "\n",
       "                               title_romaji   type  duration  start_year  \\\n",
       "0                                   Berserk  MANGA       NaN      1989.0   \n",
       "1  JoJo no Kimyou na Bouken: Steel Ball Run  MANGA       NaN      2004.0   \n",
       "2                        Gintama: THE FINAL  ANIME     104.0      2021.0   \n",
       "3                                 ONE PIECE  MANGA       NaN      1997.0   \n",
       "4                  Fruits Basket: The Final  ANIME      24.0      2021.0   \n",
       "\n",
       "   chapters  volume publishing_status country  ...  Sci-Fi  Slice of Life  \\\n",
       "0       NaN     NaN         RELEASING      JP  ...       0              0   \n",
       "1      95.0    24.0          FINISHED      JP  ...       0              0   \n",
       "2       NaN     NaN          FINISHED      JP  ...       1              0   \n",
       "3       NaN     NaN         RELEASING      JP  ...       0              0   \n",
       "4       NaN     NaN          FINISHED      JP  ...       0              1   \n",
       "\n",
       "   Sports  Supernatural  Thriller  \\\n",
       "0       0             0         0   \n",
       "1       1             1         0   \n",
       "2       0             0         0   \n",
       "3       0             0         0   \n",
       "4       0             1         0   \n",
       "\n",
       "                                title_romaji_type  \\\n",
       "0                                   Berserk_MANGA   \n",
       "1  JoJo no Kimyou na Bouken: Steel Ball Run_MANGA   \n",
       "2                        Gintama: THE FINAL_ANIME   \n",
       "3                                 ONE PIECE_MANGA   \n",
       "4                  Fruits Basket: The Final_ANIME   \n",
       "\n",
       "                                    synopsis_cleaned  synopsis_source  \\\n",
       "0  His name Guts Black Swordsman feared warrior s...       Dark Horse   \n",
       "1  Originally presented unrelated story series la...        Wikipedia   \n",
       "2  Gintama THE FINAL rd final film adaptation rem...         no match   \n",
       "3  As child Monkey D Luffy inspired become pirate...        Viz Media   \n",
       "4  After last season revelations Soma family move...       Funimation   \n",
       "\n",
       "   synopsis_wc                             synopsis_cleaned_token  \n",
       "0          425  ['name', 'feared', 'warrior', 'spoken', 'whisp...  \n",
       "1          346  ['presented', 'unrelated', 'story', 'series', ...  \n",
       "2           82  ['rd', 'final', 'film', 'adaptation', 'remaind...  \n",
       "3          348  ['child', 'inspired', 'become', 'pirate', 'lis...  \n",
       "4          277  ['last', 'season', 'revelations', 'family', 'm...  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "df_titles = pd.read_csv(\"../assets/titles_200p_cleaned.csv\")\n",
    "display(df_titles.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b48e2",
   "metadata": {},
   "source": [
    "### ↓Below is the code to vectorize synopsis text by GPT2. Commented out as it takes a bit of time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edffa778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at:  (998, nan)\n",
      "Error at:  (3183, nan)\n",
      "Error at:  (5716, nan)\n",
      "Error at:  (5906, nan)\n",
      "Error at:  (6072, nan)\n",
      "Error at:  (6329, nan)\n",
      "Error at:  (6635, nan)\n",
      "Error at:  (6954, nan)\n",
      "Error at:  (7224, nan)\n",
      "Error at:  (7422, nan)\n",
      "Error at:  (7632, nan)\n",
      "(8775, 768)\n",
      "<class 'numpy.ndarray'>\n",
      "length of df_titles:  8786\n",
      "length of error indices:  11\n",
      "[['type' 'Action' 'Adventure' ... 'Slice of Life' 'title_romaji'\n",
      "  'synopsis_source']\n",
      " ['MANGA' 1 1 ... 0 'Berserk' 'Dark Horse']\n",
      " ['MANGA' 1 1 ... 0 'JoJo no Kimyou na Bouken: Steel Ball Run'\n",
      "  'Wikipedia']\n",
      " ...\n",
      " ['MANGA' 0 0 ... 0 'Katappashi Kara Zenbu Koi' 'no match']\n",
      " ['ANIME' 0 0 ... 0 'WiSH VOYAGE' 'no match']\n",
      " ['MANGA' 0 1 ... 0\n",
      "  'Maydare Tensei Monogatari: Kono Sekai de Ichiban Warui Majo'\n",
      "  'no match']]\n"
     ]
    }
   ],
   "source": [
    "max_length = 256\n",
    "vec = [] # destination of sentence vectors\n",
    "error_idx = [] # indices to exclude from visualization\n",
    "\n",
    "for item in df_titles[\"synopsis_cleaned\"].iteritems():\n",
    "    try:\n",
    "        tokenizer.pad_token = tokenizer.eos_token # https://github.com/huggingface/transformers/issues/12594\n",
    "        enc = tokenizer(\n",
    "            item[1],\n",
    "            max_length=max_length,\n",
    "            padding=True, # https://github.com/huggingface/transformers/issues/2630 -> https://huggingface.co/docs/transformers/glossary#attention-mask\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = { k: v.to(device) for k, v in enc.items() }\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "        with torch.no_grad():\n",
    "            output = model(**enc)\n",
    "            last_hs = output.last_hidden_state\n",
    "            averaged_hs = (last_hs*attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)\n",
    "\n",
    "        vec.append(averaged_hs[0].cpu().numpy())\n",
    "    except:\n",
    "        error_idx.append(item[0]+1)\n",
    "        print(\"Error at: \", item)\n",
    "\n",
    "vec = np.array(vec)\n",
    "print(vec.shape)\n",
    "print(type(vec))\n",
    "\n",
    "print(\"length of df_titles: \", len(df_titles))\n",
    "print(\"length of error indices: \", len(error_idx))\n",
    "meta_header = prep_label_array(df_titles, error_idx, [\"type\", \"Action\", \"Adventure\", \"Drama\", \"Mystery\", \"Psychological\", \"Romance\", \"Slice of Life\", \"title_romaji\", \"synopsis_source\"])\n",
    "print(meta_header)\n",
    "np.savetxt(\"../assets/synvec_gpt2.tsv\", vec, delimiter='\\t', fmt=\"%f\")\n",
    "np.savetxt(\"../assets/synvec_gpt2_meta.tsv\", meta_header, delimiter='\\t', fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44201ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gpt2 vectors\n",
    "vec = np.loadtxt(\"../assets/synvec_gpt2.tsv\", delimiter='\\t')\n",
    "\n",
    "# tensorboard\n",
    "writer = SummaryWriter()\n",
    "writer.add_embedding(torch.FloatTensor(vec))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0d48805",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-77952b0f7cb34813\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-77952b0f7cb34813\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizations by tensorboard\n",
    "# Download the \"../assets/synvec_gpt2_meta.tsv\" file and \"Load\" data to label data points\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd9191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
